{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for SPOC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "papers = pd.read_json(\"../data/papers.json\")\n",
    "papers_tei = pathlib.Path(\"/Users/jpnelson/Google Drive/Shared drives/SUL AI 2020-2021/Project - Species Occurrences/papers_tei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "papers.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Full-Text from TEI XML\n",
    "[GROBID](https://grobid.readthedocs.io/en/latest/) extracts the full-text from the PDFs and saves the result in a\n",
    "[TEI](https://tei-c.org/) XML document. We then use XPath to extract the full-text within the document.\n",
    "\n",
    "First, we will create a TEI namespace to simplify the construction of the XPath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEI = { 'tei': 'http://www.tei-c.org/ns/1.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WorMS Marine Species Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy_lookup import Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon = pd.read_csv('/Users/jpnelson/Google Drive/Shared drives/SUL AI 2020-2021/Project - Species Occurrences/WoRMS_marineSpecies/taxon.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_id_names = {}\n",
    "for row in taxon.iterrows():\n",
    "    sci_id_names[row[1]['taxonID']] = [row[1]['scientificName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfix_imports\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ASCII'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbuffers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Read and return an object from the pickle data stored in a file.\n",
       "\n",
       "This is equivalent to ``Unpickler(file).load()``, but may be more\n",
       "efficient.\n",
       "\n",
       "The protocol version of the pickle is detected automatically, so no\n",
       "protocol argument is needed.  Bytes past the pickled object's\n",
       "representation are ignored.\n",
       "\n",
       "The argument *file* must have two methods, a read() method that takes\n",
       "an integer argument, and a readline() method that requires no\n",
       "arguments.  Both methods should return bytes.  Thus *file* can be a\n",
       "binary file object opened for reading, an io.BytesIO object, or any\n",
       "other custom object that meets this interface.\n",
       "\n",
       "Optional keyword arguments are *fix_imports*, *encoding* and *errors*,\n",
       "which are used to control compatibility support for pickle stream\n",
       "generated by Python 2.  If *fix_imports* is True, pickle will try to\n",
       "map the old Python 2 names to the new names used in Python 3.  The\n",
       "*encoding* and *errors* tell pickle how to decode 8-bit string\n",
       "instances pickled by Python 2; these default to 'ASCII' and 'strict',\n",
       "respectively.  The *encoding* can be 'bytes' to read these 8-bit\n",
       "string instances as bytes objects.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/scientific-names.pkl\", '+wb') as fo:\n",
    "    pickle.dump(sci_id_names, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fd5bcaa6ca0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci_names_entity = Entity(keywords_dict=sci_id_names, label=\"SPECIES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_locations = pd.read_csv('/Users/jpnelson/Google Drive/Shared drives/SUL AI 2020-2021/Project - Species Occurrences/data/NamedPlaces_CA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31147 entries, 0 to 31146\n",
      "Data columns (total 20 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   FEATURE_ID       31147 non-null  int64  \n",
      " 1   FEATURE_NAME     31147 non-null  object \n",
      " 2   FEATURE_CLASS    31147 non-null  object \n",
      " 3   STATE_ALPHA      31147 non-null  object \n",
      " 4   STATE_NUMERIC    31147 non-null  int64  \n",
      " 5   COUNTY_NAME      31147 non-null  object \n",
      " 6   COUNTY_NUMERIC   31147 non-null  int64  \n",
      " 7   PRIMARY_LAT_DMS  31147 non-null  object \n",
      " 8   PRIM_LONG_DMS    31147 non-null  object \n",
      " 9   PRIM_LAT_DEC     31147 non-null  float64\n",
      " 10  PRIM_LONG_DEC    31147 non-null  float64\n",
      " 11  SOURCE_LAT_DMS   6791 non-null   object \n",
      " 12  SOURCE_LONG_DMS  6791 non-null   object \n",
      " 13  SOURCE_LAT_DEC   6791 non-null   float64\n",
      " 14  SOURCE_LONG_DEC  6791 non-null   float64\n",
      " 15  ELEV_IN_M        30645 non-null  float64\n",
      " 16  ELEV_IN_FT       30645 non-null  float64\n",
      " 17  MAP_NAME         31147 non-null  object \n",
      " 18  DATE_CREATED     31147 non-null  object \n",
      " 19  DATE_EDITED      8627 non-null   object \n",
      "dtypes: float64(6), int64(3), object(11)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "ca_locations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_locations = pd.read_csv('/Users/jpnelson/Google Drive/Shared drives/SUL AI 2020-2021/Project - Species Occurrences/data/NamedPlaces_OR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEATURE_ID</th>\n",
       "      <th>FEATURE_NAME</th>\n",
       "      <th>FEATURE_CLASS</th>\n",
       "      <th>STATE_ALPHA</th>\n",
       "      <th>STATE_NUMERIC</th>\n",
       "      <th>COUNTY_NAME</th>\n",
       "      <th>COUNTY_NUMERIC</th>\n",
       "      <th>PRIMARY_LAT_DMS</th>\n",
       "      <th>PRIM_LONG_DMS</th>\n",
       "      <th>PRIM_LAT_DEC</th>\n",
       "      <th>PRIM_LONG_DEC</th>\n",
       "      <th>SOURCE_LAT_DMS</th>\n",
       "      <th>SOURCE_LONG_DMS</th>\n",
       "      <th>SOURCE_LAT_DEC</th>\n",
       "      <th>SOURCE_LONG_DEC</th>\n",
       "      <th>ELEV_IN_M</th>\n",
       "      <th>ELEV_IN_FT</th>\n",
       "      <th>MAP_NAME</th>\n",
       "      <th>DATE_CREATED</th>\n",
       "      <th>DATE_EDITED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1116450</td>\n",
       "      <td>Switch Back Creek</td>\n",
       "      <td>Stream</td>\n",
       "      <td>OR</td>\n",
       "      <td>41</td>\n",
       "      <td>Lane</td>\n",
       "      <td>39</td>\n",
       "      <td>440055N</td>\n",
       "      <td>1221714W</td>\n",
       "      <td>44.015401</td>\n",
       "      <td>-122.287268</td>\n",
       "      <td>440138N</td>\n",
       "      <td>1221620W</td>\n",
       "      <td>44.027222</td>\n",
       "      <td>-122.272222</td>\n",
       "      <td>868.0</td>\n",
       "      <td>2848.0</td>\n",
       "      <td>Harvey Mountain</td>\n",
       "      <td>6/1/92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1116452</td>\n",
       "      <td>Dearborn Island</td>\n",
       "      <td>Island</td>\n",
       "      <td>OR</td>\n",
       "      <td>41</td>\n",
       "      <td>Lane</td>\n",
       "      <td>39</td>\n",
       "      <td>440959N</td>\n",
       "      <td>1221434W</td>\n",
       "      <td>44.166514</td>\n",
       "      <td>-122.242839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>364.0</td>\n",
       "      <td>1194.0</td>\n",
       "      <td>McKenzie Bridge</td>\n",
       "      <td>6/1/92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1116453</td>\n",
       "      <td>Gold Basin Springs</td>\n",
       "      <td>Spring</td>\n",
       "      <td>OR</td>\n",
       "      <td>41</td>\n",
       "      <td>Curry</td>\n",
       "      <td>15</td>\n",
       "      <td>421817N</td>\n",
       "      <td>1235344W</td>\n",
       "      <td>42.304740</td>\n",
       "      <td>-123.895584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>3901.0</td>\n",
       "      <td>Tincup Peak</td>\n",
       "      <td>6/1/92</td>\n",
       "      <td>1/2/13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1116454</td>\n",
       "      <td>Cedar Camp</td>\n",
       "      <td>Locale</td>\n",
       "      <td>OR</td>\n",
       "      <td>41</td>\n",
       "      <td>Curry</td>\n",
       "      <td>15</td>\n",
       "      <td>421611N</td>\n",
       "      <td>1235944W</td>\n",
       "      <td>42.269830</td>\n",
       "      <td>-123.995637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>978.0</td>\n",
       "      <td>3209.0</td>\n",
       "      <td>Tincup Peak</td>\n",
       "      <td>6/1/92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1116473</td>\n",
       "      <td>Jacks Camp</td>\n",
       "      <td>Locale</td>\n",
       "      <td>OR</td>\n",
       "      <td>41</td>\n",
       "      <td>Curry</td>\n",
       "      <td>15</td>\n",
       "      <td>421801N</td>\n",
       "      <td>1240319W</td>\n",
       "      <td>42.300386</td>\n",
       "      <td>-124.055362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>811.0</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>Big Craggies</td>\n",
       "      <td>6/1/92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FEATURE_ID        FEATURE_NAME FEATURE_CLASS STATE_ALPHA  STATE_NUMERIC  \\\n",
       "0     1116450   Switch Back Creek        Stream          OR             41   \n",
       "1     1116452     Dearborn Island        Island          OR             41   \n",
       "2     1116453  Gold Basin Springs        Spring          OR             41   \n",
       "3     1116454          Cedar Camp        Locale          OR             41   \n",
       "4     1116473          Jacks Camp        Locale          OR             41   \n",
       "\n",
       "  COUNTY_NAME  COUNTY_NUMERIC PRIMARY_LAT_DMS PRIM_LONG_DMS  PRIM_LAT_DEC  \\\n",
       "0        Lane              39         440055N      1221714W     44.015401   \n",
       "1        Lane              39         440959N      1221434W     44.166514   \n",
       "2       Curry              15         421817N      1235344W     42.304740   \n",
       "3       Curry              15         421611N      1235944W     42.269830   \n",
       "4       Curry              15         421801N      1240319W     42.300386   \n",
       "\n",
       "   PRIM_LONG_DEC SOURCE_LAT_DMS SOURCE_LONG_DMS  SOURCE_LAT_DEC  \\\n",
       "0    -122.287268        440138N        1221620W       44.027222   \n",
       "1    -122.242839            NaN             NaN             NaN   \n",
       "2    -123.895584            NaN             NaN             NaN   \n",
       "3    -123.995637            NaN             NaN             NaN   \n",
       "4    -124.055362            NaN             NaN             NaN   \n",
       "\n",
       "   SOURCE_LONG_DEC  ELEV_IN_M  ELEV_IN_FT         MAP_NAME DATE_CREATED  \\\n",
       "0      -122.272222      868.0      2848.0  Harvey Mountain       6/1/92   \n",
       "1              NaN      364.0      1194.0  McKenzie Bridge       6/1/92   \n",
       "2              NaN     1189.0      3901.0      Tincup Peak       6/1/92   \n",
       "3              NaN      978.0      3209.0      Tincup Peak       6/1/92   \n",
       "4              NaN      811.0      2661.0     Big Craggies       6/1/92   \n",
       "\n",
       "  DATE_EDITED  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2      1/2/13  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_locations = pd.read_csv('/Users/jpnelson/Google Drive/Shared drives/SUL AI 2020-2021/Project - Species Occurrences/data/NamedPlaces_WA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.concat([ca_locations, or_locations, wa_locations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dict = dict(zip(locations.FEATURE_ID, locations.FEATURE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in locations_dict.keys():\n",
    "    place_list = [locations_dict[key],]\n",
    "    locations_dict[key] = place_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_entity = Entity(keywords_dict=locations_dict, label=\"LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_entity.name = 'location_entity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'location_entity'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_entity.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/locations.pkl\", \"wb\") as fo:\n",
    "    pickle.dump(locations_dict, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habitat DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "habitats = []\n",
    "with open(\"/Users/jpnelson/Google Drive/Shared drives/SUL AI 2020-2021/Project - Species Occurrences/data/habitats.jsonl\") as fo:\n",
    "    for row in fo.readlines():\n",
    "        line = json.loads(row)\n",
    "        habitats.append(line.get('pattern'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "habitats_entity = Entity(keywords_list=habitats, label=\"HABITAT\")\n",
    "habitats_entity.name = 'habitat_entity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct spaCy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fd56f8b4640>)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(sci_names_entity)\n",
    "nlp.add_pipe(locations_entity)\n",
    "nlp.add_pipe(habitats_entity)\n",
    "nlp.remove_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "pz596wd3318 = papers_tei/\"hms_pz596wd3318.tei.xml\"\n",
    "pz596wd3318_xml = etree.XML(pz596wd3318.read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_date = pz596wd3318_xml.find(\"tei:teiHeader/tei:fileDesc/tei:publicationStmt/tei:date\", namespaces=TEI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1996-06-06'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_date.attrib.get('when', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml2records(xml_path: pathlib.Path) -> pd.DataFrame:\n",
    "    # Create an XML document from xml path\n",
    "    tei_xml = etree.XML(xml_path.read_bytes())\n",
    "    divs = tei_xml.findall(\"tei:text/tei:body/tei:div\", namespaces=TEI)\n",
    "    # Extracts Publication Date if it exists\n",
    "    pub_date_element = tei_xml.find(\"tei:teiHeader/tei:fileDesc/tei:publicationStmt/tei:date\", namespaces=TEI)\n",
    "    # Records in the paper\n",
    "    records = []\n",
    "    if pub_date_element is not None:\n",
    "        pub_date = pub_date_element.attrib.get('when', '')\n",
    "    else:\n",
    "        pub_date = ''\n",
    "    # Iterate over each div, retreive all of the text, and run NER\n",
    "    for i,div in enumerate(divs):\n",
    "        text = ''\n",
    "        for row in div.itertext():\n",
    "            text += f\" {row}\"\n",
    "        doc = nlp(text)\n",
    "        species, places, habitats = [],[],[]\n",
    "        # Iterate through document's entities and add to lists\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_.startswith(\"SPECIES\"):\n",
    "                species.append(ent)\n",
    "            elif ent.label_.startswith(\"LOCATION\"):\n",
    "                places.append(ent.text)\n",
    "            elif ent.label_.startswith(\"HABITAT\"):\n",
    "                habitats.append(ent.text)\n",
    "        # Get list of species entities with IDs\n",
    "        species = [(sci_names_entity.keyword_processor.get_keyword(ent.text), ent.text) for ent in species]\n",
    "        species = list(set(species))\n",
    "        # Removes duplicates from locations and habit\n",
    "        locations = list(set(places))\n",
    "        habitats = list(set(habitats))\n",
    "        div_number = i + 1\n",
    "        for row in species:\n",
    "            record = { 'Paper ID': xml_path.name,\n",
    "                       'Instance ID': row[0],\n",
    "                       'Species': row[1],\n",
    "                       'GBIF': f\"https://www.gbif.org/species/search?q={row[0]}&qField=SCIENTIFIC\",\n",
    "                       'Time': pub_date,\n",
    "                       'Place': ','.join(locations),\n",
    "                       'Habitats': ','.join(habitats),\n",
    "                       'div_enum': div_number\n",
    "                       }\n",
    "            records.append(record)\n",
    "    return pd.DataFrame(records)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start conversion at 2021-03-05 18:19:48.792341\n",
      "...25.Error with fhl_2011_Clark_26624.tei.xml\n",
      "..50..75...100..125...150..175.Error with fhl_2012_Townsend_27041.tei.xml\n",
      ".200..225...250..275.Error with fhl_2014_Cougan_27609.tei.xml\n",
      "..300..325...350..375...400..425...450..475...500..525...550..575...600..625.Error with hms_cj258ns3486.tei.xml\n",
      "..650..675...700..725...750..775...800..825...850..875...900..925.Error with hms_tb429nk3829.tei.xml\n",
      "..950.Error with hms_sf858vn4653.tei.xml\n",
      ".975..Error with hms_vp112bk1362.tei.xml\n",
      ".1000..Error with hms_vz980wk8856.tei.xml\n",
      "...1050..1075Error with hms_zt342rt9331.tei.xml\n",
      ".Error with hms_zx897dq3818.tei.xml\n",
      "Error with hms_zz467pc7918.tei.xml\n",
      ".1100..1125..Error with hms_td830hd7966.tei.xml\n",
      "..1175...1200..1225...1250Error with hms_zd610ry2635.tei.xml\n",
      "..1275...1300..1325...1350..1375...1400..1425...1450..1475...1500..1525...1550..1575...1600..1625.Finished at 2021-03-05 18:25:24.880164, total time 5.6 minutes\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.utcnow()\n",
    "all_records = None\n",
    "print(f\"Start conversion at {start}\")\n",
    "for i, tei_path in enumerate(papers_tei.iterdir()):\n",
    "    try:\n",
    "        records = xml2records(tei_path)\n",
    "    except:\n",
    "        print(f\"Error with {tei_path.name}\")\n",
    "        continue\n",
    "    if all_records is None:\n",
    "        all_records = records\n",
    "    else:\n",
    "        all_records = pd.concat([all_records, records], ignore_index=True)\n",
    "    if not i%10:\n",
    "        print(\".\", end=\"\")\n",
    "    if not i%25 and i > 0:\n",
    "        print(f\"{i}\", end=\"\")\n",
    "end = datetime.datetime.utcnow()\n",
    "print(f\"Finished at {end}, total time {(end-start).seconds / 60.} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records.to_json(\"../data/species-records.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E103] Trying to set conflicting doc.ents: '(135, 136, 'SPECIES')' and '(134, 136, 'LOCATION')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-329-a89e13888967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfhl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers_tei\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"fhl_2012_Townsend_27041.tei.xml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfhl_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxml2records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfhl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-324-f1ad274c238e>\u001b[0m in \u001b[0;36mxml2records\u001b[0;34m(xml_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" {row}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mspecies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhabitats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Iterate through document's entities and add to lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/02021/sul-dlss/labs/ml-env/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/02021/sul-dlss/labs/ml-env/lib/python3.9/site-packages/spacy_lookup/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Overwrite doc.ents and add entity – be careful not to replace!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(135, 136, 'SPECIES')' and '(134, 136, 'LOCATION')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap."
     ]
    }
   ],
   "source": [
    "fhl = papers_tei/\"fhl_2012_Townsend_27041.tei.xml\"\n",
    "fhl_df = xml2records(fhl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/jpnelson/02021/sul-dlss/labs/spoc/doc/doc.pyx\u001b[0m(578)\u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  doc.ents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'doc' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  u\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/jpnelson/02021/sul-dlss/labs/ml-env/lib/python3.9/site-packages/spacy_lookup/__init__.py\u001b[0m(62)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     60 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     61 \u001b[0;31m        \u001b[0;31m# Overwrite doc.ents and add entity – be careful not to replace!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 62 \u001b[0;31m        \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     63 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     64 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  doc.ents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Clara, Clara, bacteria)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  spans\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elwha, Santa Clara, Santa Clara, delta, Eel River, delta, delta, delta]\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sci_id_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8c022652e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msci_id_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sci_id_names' is not defined"
     ]
    }
   ],
   "source": [
    "len(sci_id_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
